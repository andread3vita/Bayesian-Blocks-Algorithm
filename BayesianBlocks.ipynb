{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Andrea De Vita\n",
    "- Enrico Lupi\n",
    "\n",
    "-----------------------\n",
    "\n",
    "# Bayesian Blocks Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(warn=-1)\n",
    "library(tidyverse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The Bayesian Block algorithm exploits a simple non-parametric modeling technique to identify and characterize statistically significant variations, and at the same time suppress the corrupting observational errors. It was originally developed for astronomy applications to detect local variability in time series, but has since found use also in high energy physics as a way to impove the binning of histograms.  \n",
    "\n",
    "After a brief introduction on the algorithm, we present its implementation in R and quantitatively compare its performance with other commonly used binning methods. Lastly, we apply it to some HEP distributions to show its usefulness in real-life scenarios. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "1. [Introduction](#introduction) <br>\n",
    "2. [Algorithm](#algorithm) <br>\n",
    "    2.1. [Algorithm Description](#alg_description) <br>\n",
    "    2.1. [Code Implementation](#alg_implementation) <br>\n",
    "    2.1. [Execution Time](#alg_time) <br>\n",
    "3. [Comparison with Other Binning Methods](#comparison) <br>\n",
    "    3.1. [Metrics](#comp_metrics) <br>\n",
    "    3.2. [Results](#comp_results) <br>\n",
    "4. [Applications to Nuclear and Subnuclear Physics](#applications) <br>\n",
    "    4.1. [Radioactive Source](#app_radio) <br>\n",
    "    4.2. [Higgs Physics](#app_higgs) <br>\n",
    "5. [Bibliography](#bibliography) <br>\n",
    "\n",
    "\n",
    "-----------------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a name=\"introduction\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm <a name=\"algorithm\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Algorithm Description <a name=\"alg_description\"></a>\n",
    "\n",
    "We will follow a *dynamic programming* approach. Beginning with the first data cell, at each step one more cell is added using the results of the previous steps: the value of optimal fitness is stored in the array *best* and the location of the last change point in the array *last*.\n",
    "\n",
    "Let $\\mathcal{P}^{opt}(R)$ denote the optimal partition of the first $R$ cells. In the starting case $R = 1$, the only possible partition (one block consisting of the first cell by itself) is trivially optimal. Now assume we have completed step $R$: it remains to show how to obtain $\\mathcal{P}^{opt}(R+1)$.  \n",
    "For some $r$ consider the set of all partitions (of these first R+1 cells) whose last block starts with cell $r$ (and by definition ends at R + 1) and denote the fitness of this last block by $F(r)$. The only member of this set that could possibly be optimal is that consisting of $\\mathcal{P}^{opt}(r-1)$ followed by this last block. By the additivity of the Likelihood the fitness of said partition is the sum of $F(r)$ and the fitness of $\\mathcal{P}^{opt}(r-1)$ (saved from a previous step in *best*):\n",
    "\n",
    "$$ \n",
    "A(r) = F(r) + \n",
    "\\begin{cases}\n",
    "    0, & r = 1\\\\\n",
    "    best(r - 1), & r = 2, 3, . . . , R + 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The value of $r$ yielding the optimal partition $\\mathcal{P}^{opt}(R+1)$ is the value maximizing $A(r)$:\n",
    "\n",
    "$$\n",
    "r^{opt} = argmax[A(r)]\n",
    "$$\n",
    "\n",
    "\n",
    "At the end of this computation, when $R = N$, it only remains to find the locations of the change points of the optimal partition. The needed information is contained in the array *last*: we use the last value in this array to determine the last change point in $\\mathcal{P}^{opt}(N)$, peel off the end section of last corresponding to this last block, and repeat:\n",
    "\n",
    "$$\n",
    "cp1 = last(N) \\\\\n",
    "cp2 = last(cp1 − 1) \\\\\n",
    "cp3 = last(cp2 − 1) \\\\\n",
    "...\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Code Implementation <a name=\"alg_implementation\"></a>\n",
    "\n",
    "Here is the R code implementing the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "bayesian_blocks <- function(data,\n",
    "                            weights = NULL,\n",
    "                            Prior = \"calibrated\",\n",
    "                            p0 = 0.05,\n",
    "                            gamma = NULL) {\n",
    "    # Bayesian Blocks Implementation.\n",
    "\n",
    "    # This is a flexible implementation of the Bayesian Blocks algorithm for 1 dimension.\n",
    "\n",
    "    # Args:\n",
    "    #   data: Input data values (one dimensional, length N). Repeat values are allowed.\n",
    "\n",
    "    #   weights: Weights for data (otherwise assume all data points have a weight of 1).\n",
    "    #            Must be same length as data. Defaults to NULL.\n",
    "\n",
    "    #   prior: Prior on the number of blocks. Options include a uniform prior, a gemetric prior that\n",
    "    #          penalizes an excessive number of blocks and a calibrated prior in order to achieve the\n",
    "    #          specified false-positive rate p0. Deafults to the calibrated one. \n",
    "\n",
    "    #   p0: False-positive rate, between 0 and 1. A lower number places a stricter penalty\n",
    "    #       against creating more bin edges, thus reducing the potential for false-positive bin edges. In general,\n",
    "    #       the larger the number of bins, the small the p0 should be to prevent the creation of spurious, jagged\n",
    "    #       bins. Defaults to 0.05.\n",
    "\n",
    "    #   gamma: If specified, can use this gamma to compute the general prior form,\n",
    "    #          p = gamma^N. Defaults to NULL.\n",
    "\n",
    "    # Returns:\n",
    "    #   Array containing the (N+1) bin edges\n",
    "\n",
    "    # Examples:\n",
    "    #   Unweighted data:\n",
    "\n",
    "    #   >>> d <- rnorm(n=100)\n",
    "    #   >>> bins <- bayesian_blocks(d, p0=0.01)\n",
    "\n",
    "    #   Unweighted data with repeats:\n",
    "\n",
    "    #   >>> d <- rnorm(n=100)\n",
    "    #   >>> d[81:100] <- d[1:20]\n",
    "    #   >>> bins <- bayesian_blocks(d, p0=0.01)\n",
    "\n",
    "    #   Weighted data:\n",
    "\n",
    "    #   >>> d <- rnorm(n=100)\n",
    "    #   >>> w <- runif(n=100, min=1, max=2)\n",
    "    #   >>> bins <- bayesian_blocks(d, w, p0=0.01)\n",
    "\n",
    "    \n",
    "    # validate input data\n",
    "    data <- as.vector(data)\n",
    "\n",
    "    # validate input weights\n",
    "    if (!is.null(weights)) {\n",
    "        weights <- as.vector(weights)\n",
    "    }\n",
    "    else {\n",
    "        # set them to 1 if not given\n",
    "        weights <- rep(1, length(data))\n",
    "    }\n",
    "\n",
    "    # Place data and weights into a DataFrame.\n",
    "    # We want to sort the data array (without losing the associated weights), and combine duplicate\n",
    "    # data points by summing their weights together.\n",
    "    df <- data.frame(data = data, weights = weights) |>\n",
    "          group_by(data) |>\n",
    "          summarise(sum_w = sum(weights))\n",
    "    data <- pull(df, data)\n",
    "    weights <- pull(df, sum_w)\n",
    "\n",
    "    N = length(data)\n",
    "\n",
    "\n",
    "    # create length-(N + 1) array of cell edges\n",
    "    edges <- c(data[1], 0.5 * (data[1:(N-1)] + data[2:N]), data[N])\n",
    "    block_length = data[N] - edges\n",
    "\n",
    "\n",
    "    # arrays to store the best configuration\n",
    "    best <- rep(0, N)\n",
    "    last <- rep(0, N)\n",
    "\n",
    "\n",
    "    # compute prior\n",
    "    prior <- switch(Prior, \n",
    "                    \"uniform\" = 0,\n",
    "                    \"gamma\" = -log(gamma),\n",
    "                    \"calibrated\" = 4 - log(73.53 * p0 * N**(-0.478)),\n",
    "                    0 # default case\n",
    "             )\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Start with first data cell; add one cell at each iteration\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    for (R in 1:N) {\n",
    "        # Compute fit_vec : fitness of putative last block (ends at data cell R/edge R+1)\n",
    "\n",
    "        # T_k: width/duration of each block\n",
    "        T_k <- block_length[1:R] - block_length[R+1]\n",
    "\n",
    "        # N_k: number of elements in each block\n",
    "        N_k <- rev(cumsum(rev(weights[1:R])))\n",
    "\n",
    "        # evaluate fitness function using Cash statistics\n",
    "        fit_vec <- N_k * log(N_k / T_k)\n",
    "\n",
    "        # penalize function with prior\n",
    "        A_R <- fit_vec - prior\n",
    "        if(R > 1) {\n",
    "            A_R[2:R] <- A_R[2:R] + best[1:(R-1)]\n",
    "        }\n",
    "        i_max = which.max(A_R)\n",
    "        last[R] = i_max\n",
    "        best[R] = A_R[i_max]\n",
    "    }\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Now find changepoints by iteratively peeling off the last block\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    change_points = rep(0, N)\n",
    "    i_cp = N + 1\n",
    "    ind = N + 1\n",
    "    repeat {\n",
    "        i_cp <- i_cp - 1\n",
    "        change_points[i_cp] <- ind\n",
    "        if(ind == 1) {\n",
    "            break\n",
    "        }\n",
    "        ind <- last[ind-1]\n",
    "    }\n",
    "    change_points = change_points[i_cp:N]\n",
    "\n",
    "    return(edges[change_points])\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Execution Time <a name=\"alg_time\"></a>\n",
    "\n",
    "The total number of possible partitions (i.e., the number of ways N cells can be arranged in blocks) is $2^N$, rendering an explicit exhaustive search of partition space impossible in the general case. This algorithm, instead, is able to find the optimal partition in time of order $O(N^2)$, and is practical even for $N ∼ 1,000,000$, for which approximately $10^{300,000}$ partitions are possible.  \n",
    "\n",
    "We will verify this scaling law by computing the execution time of the algorithm over datasets of increasing size over different order of magnitude. For simplicity, the data is generated form a Normal distribution \n",
    "${\\mathcal {N}}(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Test execution time\n",
    "compute_time <- Vectorize(function(N, func, time_unit = \"sec\", seed = 48) {\n",
    "    # create dummy dataset\n",
    "    set.seed(seed)\n",
    "    data <- rnorm(N, mean = 0, sd = 1)\n",
    "\n",
    "    # start clock\n",
    "    start <- Sys.time()\n",
    "\n",
    "    func(data)\n",
    "\n",
    "    # end clock\n",
    "    end <- Sys.time()\n",
    "    exec_time <- as.numeric(difftime(end, start, units = time_unit))\n",
    "    return(exec_time)\n",
    "}, vectorize.args = \"N\") \n",
    "\n",
    "\n",
    "# Compute execution time over different ranges\n",
    "N1 <- c(1e2, 1e3, 2e3, 3e3, 4e3, 5e3, 6e3, 7e3, 8e3, 9e3, 1e4)\n",
    "N2 <- c(1e3, round(sqrt(1e7)), 1e4, round(sqrt(1e9)), 1e5)\n",
    "\n",
    "exec_time1 <- compute_time(N1, bayesian_blocks)\n",
    "exec_time2 <- compute_time(N2, bayesian_blocks)\n",
    "\n",
    "\n",
    "# Quadratic model fit\n",
    "fit_data1 <- data.frame(x = N1, x2 = N1**2, y = exec_time1)\n",
    "fit_data2 <- data.frame(x = N2, x2 = N2**2, y = exec_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "fitted_func <- function(fit, x) {\n",
    "      res <- as.numeric(fit$coefficients[\"(Intercept)\"]) +\n",
    "             as.numeric(fit$coefficients[\"x\"]) * x +\n",
    "             as.numeric(fit$coefficients[\"x2\"]) * x**2\n",
    "      return(res)\n",
    "}\n",
    "\n",
    "par(mfrow = c(1, 2))\n",
    "options(repr.plot.width=16, repr.plot.height=8)\n",
    "\n",
    "plot(N1, exec_time1, xlab = \"N\", ylab = \"Execution Time [s]\", col = \"blue\", pch = 10, lwd = 3)\n",
    "grid()\n",
    "curve(fitted_func(fit1, x), N1[1], N1[length(N1)],\n",
    "      col = \"red\", lw = 2, add = T)\n",
    "legend(\"topleft\", inset=+0.1, legend = c(\"Points\", \"Quadratic Fit\"),\n",
    "      col = c(\"blue\",\"red\"), lty = c(NA, 1), pch = c(10, NA), lwd = c(3, 2),\n",
    "      cex = 0.9, box.lty = 0, border = F)\n",
    "\n",
    "plot(N2, exec_time2, xlab = \"N\", ylab = \"Execution Time [s]\", col = \"blue\", pch = 10, lwd = 3, log = \"xy\")\n",
    "grid()\n",
    "curve(fitted_func(fit2, x), N2[1], N2[length(N2)],\n",
    "      col = \"red\", lw = 2, add = T)\n",
    "legend(\"topleft\", inset=+0.1, legend = c(\"Points\", \"Quadratic Fit\"),\n",
    "      col = c(\"blue\",\"red\"), lty = c(NA, 1), pch = c(10, NA), lwd = c(3, 2),\n",
    "      cex = 0.9, box.lty = 0, border = F)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the data points follow a quadratic distribution quite nicely, with some slight deviations at lower $N$ for linear and constant effects."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison with Other Binning Methods <a name=\"comparison\"></a>\n",
    "\n",
    "Other objective methods have been proposed to determine binning according to some optimization procedure.\n",
    "We will now compare the Bayesian Block algorithm using different priors with the following:\n",
    "\n",
    "- Square root: $K = \\lceil \\sqrt{N} \\rceil$\n",
    "- Sturges: $K = \\lceil ln(N) \\rceil + 1$  [[3]](#Sturges)\n",
    "- Freedman-Diaconis: $h = 2 \\frac{IQRi}{N^{1/3}}$, where $IQR$ is the interquartile range of the data  [[4]](#FreedmanDiaconis)\n",
    "\n",
    "where $K$ is the number of bins and $h$ is their width."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Metrics <a name=\"comp_metrics\"></a>\n",
    "\n",
    "In order to quantitavely compare different methods we will use two metrics, introduced by Pollack. [[2]](#Pollack)\n",
    "\n",
    "The first metric is designed to capture the visual appeal of the histogram by minimizing the number of bin-to-bin height fluctuations, hereafter referred to as \"wiggles\", that denote unwanted statistical fluctuations. The number of wiggles in a histogram is defined as:\n",
    "\n",
    "$$ W_n = \\sum \\left[ sgn \\left( f'\\left(B_i \\right) \\right) \\cdot sgn \\left( f'\\left(B_{i+1} \\right) \\right) = -1 \\right]$$\n",
    "\n",
    "where $f \\left(B_i \\right)$ is the finite first derivative of the function describing the height of block (or bin) $i$: this metric simply counts the number of adjacent opposite-sign first derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "### description of second metric here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Results <a name=\"comp_results\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applications to Nuclear and Subnuclear Physics  <a name=\"applications\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Radioactive Source  Spectrum <a name=\"app_radio\"></a>\n",
    "\n",
    "We will apply the algorithm to energy spectra measured by a $LaBr_3$ scintillator detector at INFN Legnaro laboratories. The following raioactive sources will be studied:\n",
    "\n",
    "- Cobalt-60\n",
    "- Cesium-137\n",
    "- Sodium-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m700325\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m2\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \"\\t\"\n",
      "\u001b[32mdbl\u001b[39m (2): ADC_channel, Energy\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "\u001b[1mRows: \u001b[22m\u001b[34m542364\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m2\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \"\\t\"\n",
      "\u001b[32mdbl\u001b[39m (2): ADC_channel, Energy\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "\u001b[1mRows: \u001b[22m\u001b[34m572864\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m2\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \"\\t\"\n",
      "\u001b[32mdbl\u001b[39m (2): ADC_channel, Energy\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "Co <- read_tsv(\"Data/Co10cm.txt\") |> pull(\"Energy\")\n",
    "Cs <- read_tsv(\"Data/Cs10cm.txt\") |> pull(\"Energy\")\n",
    "Na <- read_tsv(\"Data/Na10cm.txt\") |> pull(\"Energy\")\n",
    "\n",
    "breaks_Co <- bayesian_blocks(Co)\n",
    "breaks_Cs <- bayesian_blocks(Cs)\n",
    "breaks_Na <- bayesian_blocks(Na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "par(mfrow = c(3, 2))\n",
    "options(repr.plot.width = 16, repr.plot.height=24)\n",
    "\n",
    "hist(Co, breaks = breaks_Co, col = \"red\",  border = FALSE, density = 200,\n",
    "     xlab = \"Energy [KeV]\", ylab = \"Counts\", main = sprintf(\"Co-60\"))\n",
    "grid()\n",
    "hist(Co, breaks = \"freedman-diaconis\", col = \"blue\", border = FALSE, density=200, add = TRUE)\n",
    "     #xlab = \"Energy [KeV]\", ylab = \"Counts\", main = sprintf(\"Co-60\") \n",
    "grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Higgs Physics <a name=\"app_higgs\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bibliography <a name=\"bibliography\"></a>\n",
    "\n",
    "[2] B. Pollack *et al.*,\n",
    "    [arXiv:1708.008 10](https://arxiv.org/abs/1708.00810)\n",
    "    <a name=\"Pollack\"></a>\n",
    "\n",
    "[3] H. A. Sturges,\n",
    "    Journal of the American Statistical Association  **21**, 65 (1926),\n",
    "    https://doi.org/10.1080/01621459.1926.10502161.\n",
    "    <a name=\"Sturges\"></a>\n",
    "                                             \n",
    "[4] D. Freedman and P. Daiconis,\n",
    "    Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete **57**, 453 (1981).\n",
    "    <a name=\"FreedmanDiaconis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(48)\n",
    "norm_data <- c(rnorm(2000,0,0.8),rnorm(2000,5,0.7),rnorm(2000,7,0.5))\n",
    "N <- length(norm_data)\n",
    "breaks = bayesian_blocks(norm_data, Prior=\"calibrated\", p0 = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "hist(norm_data,breaks=\"freedman-diaconis\",freq=FALSE,col=scales::alpha('darkblue',.55),border=F, panel.first=grid(),density=200,\n",
    "     xlab='x',ylab='Density',main=sprintf('Histogram of Normal distributed variables'))\n",
    "g <- hist(norm_data,breaks=breaks,density=200,\n",
    "     freq=FALSE,col=scales::alpha('firebrick3',.5),border=F,add=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#exec_time1 <- c(0.0172479152679443, 0.133802890777588, 0.338721036911011, 0.768672943115234, \n",
    "#                1.12476801872253, 1.64819598197937, 2.45492100715637, 2.97475099563599, 3.66642284393311,\n",
    "#                4.35661911964417, 5.0487699508667)\n",
    "#exec_time2 <- c(0.0695300102233887, 0.577858924865723, 4.86060404777527, 47.8130168914795, 470.706510066986)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
