{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Andrea De Vita\n",
    "- Enrico Lupi\n",
    "\n",
    "-----------------------\n",
    "\n",
    "# Bayesian Blocks Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.1     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.4.2     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.2     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.1     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n"
     ]
    }
   ],
   "source": [
    "options(warn=-1)\n",
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The Bayesian Block algorithm exploits a simple non-parametric modeling technique to identify and characterize statistically significant variations, and at the same time suppress the corrupting observational errors. It was originally developed for astronomy applications to detect local variability in time series, but has since found use also in high energy physics as a way to impove the binning of histograms.  \n",
    "\n",
    "After an introduction on the algorithm, we present its implementation in R and quantitatively compare its performance with other commonly used binning methods. Lastly, we apply it to some real physics distributions from nuclear and subnuclear physics to show its usefulness in real-life scenarios. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "1. [Introduction](#introduction) <br>\n",
    "2. [Theory](#theory)\n",
    "3. [Applications](#application) <br>\n",
    "    3.1 [Event data]() <br>\n",
    "    3.2 [Binned data]() <br>\n",
    "    3.3 [Point measurements]() <br>\n",
    "3. [Algorithm](#algorithm) <br>\n",
    "    3.1. [Algorithm Description](#alg_description) <br>\n",
    "    3.2. [Code Implementation](#alg_implementation) <br>\n",
    "5. [TEST]() <br>\n",
    "    5.1. [Simple example]() <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1.1. [Computational time]() <br>\n",
    "    5.2.[Comparison with Other Binning Methods](#comparison) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.2.1. [Metrics](#comp_metrics) <br>\n",
    "   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.2.2. [Results](#comp_results) <br>\n",
    "    5.2. [Applications to Nuclear and Subnuclear Physics]()<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.2.1. [Radioactive Source](#app_radio) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.2.2 [Higgs Physics](#app_higgs) <br>\n",
    "6. [Bibliography](#bibliography) <br>\n",
    "\n",
    "\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of data analysis is to identify and describe statistically significant variations and features within the observed data. In this pursuit, histograms are a valuable tool for visualizing and examining the data distributions.\n",
    "\n",
    "It is common practice to select a subjective range and bin width for histograms, often driven by the desire to create visually appealing plots. However, objective approaches have been developed to determine the binning based on optimization procedures like *Scott's Rule*, which is ideal for randomly sampled data following a normal distribution. Certain methods, such as *Freedman-Diaconis Rule*, consider the distribution's structure but employ fixed-width bins. Alternatively, some methods aim for each bin to have a similar number of entries, allowing for variable bin widths while still arbitrarily selecting the location of bin edges.\n",
    "\n",
    "The Bayesian Blocks algorithm offers a distinct approach by allowing variable bin widths and determining the bin edges based on the underlying distribution's structure. Originally developed by **Jeffrey D. Scargle** for astronomy applications [[1]](#Scargle), the algorithm was designed to detect and characterize local variability in time series data. However, it can also be applied to other sequential or independent variable data. One of its applications is to enhance histogram representations by allowing flexible bin sizes determined by the data.\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\" style=\"background-color: #ADD8E6; color: #000080;\">\n",
    "  <h4 class=\"alert-heading\">Baysian blocks algorithm</h4>\n",
    "  <p>Operating within a Bayesian framework, the Bayesian Blocks algorithm is a <b>non-parametric modeling technique</b> that aims to find the optimal segmentation of a set of univariate random variables into blocks. Each block consists of consecutive data elements that meet a specific criterion.</p>\n",
    "  <hr>\n",
    "  <p class=\"mb-0\">The primary objective is to differentiate statistically significant features from random observational errors, thereby uncovering local structures in the background data by utilizing the information inherent in the data itself.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theory <a name=\"theory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Event data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For series of times of discrete events it is natural to associate one data cell with each event. The following derivation of the appropriate block fitness will elucidate exactly what information the cells must contain to allow evaluation of the fitness for the full multi-block model.\n",
    "\n",
    "In practice the event times are integer multiples of some small unit, but it is often convenient to treat them as real numbers on a continuum. For example, the fitness function is easily obtained starting with the unbinned likelihood known as the Cash statistic. If $M\\left(t,\\theta\\right)$ is a model of the time dependence of a signal the unbinned loglikelihood is\n",
    "\n",
    "$$ logL \\left( \\theta \\right) = \\sum_n logM\\left(t_n,\\theta\\right) - \\int M\\left(t,\\theta\\right)dt$$\n",
    "\n",
    "where the sum is over the events and $\\theta$ represents the model parameters. The integral is calculated over the observation interval and represents the expected number of events according to the model. In our block model, which is constant and has a single parameter, we can define $M\\left(t,\\lambda\\right) = \\lambda$. Consequently, for block k, the log-likelihood can be written as:\n",
    "\n",
    "$$ logL^{\\left(k\\right)}\\left(\\lambda\\right)=N^{\\left(k\\right)}log\\lambda - \\lambda T^{\\left(k\\right)}$$\n",
    "\n",
    "where $N\\left(k\\right)$ is the number of events in block k and $T\\left(k\\right)$ is the length of the block. The maximum of this likelihood is at $\\lambda = \\frac{N\\left(k\\right)}{T\\left(k\\right)}$, yielding\n",
    "\n",
    "$$ log L_{max}^{\\left( k\\right)} + N^{\\left(k\\right)} = N^{\\left(k\\right)}\\left( logN^{\\left(k\\right)} - logT^{\\left(k\\right)} \\right)$$\n",
    "\n",
    "The term N(k) is taken to the left side because its sum over the blocks is a constant (N, the total number of events) that is model-independent and therefore irrelevant. Additionally, note that changing the units of time, e.g., using a scale factor $\\alpha$, modifies the log-likelihood by $ −N^{\\left(k\\right)} log\\left(\\alpha\\right)$. However, this change is also irrelevant due to the same reason.\n",
    "\n",
    "Interestingly, the actual positions of the independent events within their blocks do not affect the results. The fitness function solely depends on the number of events in the block, disregarding their specific locations or the intervals between them. This outcome directly arises from the nature of the underlying independently distributed or Poisson process.\n",
    "\n",
    "In conclusion, comprehensive simulations were conducted to assess the calibration of ncp_prior using signal-free observational noise. These simulations encompassed a range of values for N (the total number of events) and the selected false positive rate, denoted as $p_0$. The results obtained from these simulations were successfully fitted to the following formula:\n",
    "\n",
    "$$ \\text{ncp_prior} = 4 -\\log \\left(73.53p_0N^{-0.478} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Binned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected count in a bin is calculated as the product of three factors: the true event rate at the detector ($\\lambda$), a dimensionless exposure factor ($e$), and the width of the bin ($W$). Therefore, the likelihood for bin $n$ follows a Poisson distribution:\n",
    "\n",
    "$$L_n=\\frac{\\left(\\lambda e_n W_n\\right)^{N_n}e^{-\\lambda e_n W_n}}{N_n!}$$\n",
    "\n",
    "In this equation, $N_n$ represents the number of events in bin $n$, $\\lambda$ is the actual event rate in counts per unit time, $e_n$ is the exposure averaged over the bin, and $W_n$ is the bin width in time units. The likelihood for block $k$ is the product of the likelihoods of all its bins:\n",
    "\n",
    "$$ L^{\\left( k \\right)} \\prod^{M^{\\left( k \\right)}}_{n=1} L_n = \\lambda^{N^{\\left( k \\right)}}e^{-ew^{\\left( k \\right)}}$$\n",
    "\n",
    "Here, $M^{\\left(k\\right)}$ represents the number of bins in block $k$, and $w^{\\left( k \\right)}$ is the sum of the bin efficiencies in the block, given by $\\sum^{M^{\\left( k \\right)}}_{n=1}w_n$. Additionally, $N^{\\left( k \\right)}$ is the total event count in the block, represented by $\\sum^{M^{\\left( k \\right)}}_{n=1}N_n$.\n",
    "\n",
    "The factor $\\frac{\\left(e_n W_n\\right)^{N_n}}{N_n!}$ has been discarded because its product over all the bins in all the blocks is a constant (depending on the data only) and therefore irrelevant to model fitness. The log-likelihood is\n",
    "\n",
    "$$ log L^{\\left( k\\right)} = N^{\\left(k\\right)}log\\lambda- \\lambda w^{\\left( k \\right)}$$\n",
    "\n",
    "This log-likelihood is identical to that of event data, where $w^{\\left( k \\right)}$ plays the role of $T^{\\left(k\\right)}$, acting as an effective block duration.\n",
    "\n",
    "It is worth noting that unbinned and binned event data share the same fitness function, with the main difference being that each bin of binned data can contain multiple events. To account for this, a weight parameter can be assigned to each bin, with a value of one when considering unbinned data.\n",
    "\n",
    "In conclusion, it is possible to define an ncp_prior function, and several simulations have shown that for binned data, the prior is not sensitive to $p_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Point measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An often encountered experimental setup involves the measurement of a signal, denoted as $s(t)$, at a series of time instances $t_n \\,\\,\\, n = 1,2,...,N$, with the aim of understanding its temporal behavior. \n",
    "\n",
    "In our analysis, the observation times $t_n$ collectively form the sampling, which can be selected arbitrarily, either as evenly spaced points or in a different manner. Additionally, we explicitly assume that the measurements taken at these times are statistically independent of one another, meaning that the observation errors are independent.\n",
    "\n",
    "The observational error at a specific time $t_n$ is characterized solely by its statistical distribution. Let's consider the case where the errors follow a normal probability distribution with a mean of zero and a given variance. If the model signal is a constant value $s = \\lambda$, the likelihood of obtaining measurement $n$ is given by:\n",
    "\n",
    "$$ L_n = \\frac{1}{\\sigma_n \\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x_n-\\lambda}{\\sigma_n}\\right)^2}$$\n",
    "\n",
    "As we assume independence among the measurements, the likelihood for block $k$ can be expressed as the product of individual likelihoods:\n",
    "\n",
    "$$ L^{\\left( k \\right)} = \\prod_n L_n$$\n",
    "\n",
    "where $n$ represents the indices corresponding to the times $t_n$ falling within block $k$.\n",
    "\n",
    "The maximum value of the log-likelihood corresponding to block $k$ is the following:\n",
    "\n",
    "$$logL^{\\left(k\\right)}_{max} =-\\frac{1}{2}\\left[ 2c_k-\\frac{b_k^2}{2a_k}\\right]$$\n",
    "\n",
    "Here, the terms $a_k$, $b_k$, and $c_k$ are defined as:\n",
    "\n",
    "$$ a_k = \\frac{1}{2}\\sum_n\\frac{1}{\\sigma^2_n}$$\n",
    "$$ b_k =-\\sum_n\\frac{x_n}{\\sigma^2_n}$$\n",
    "$$ c_k =\\frac{1}{2}\\sum_n\\frac{x_n^2}{\\sigma^2_n}$$\n",
    "\n",
    "By neglecting the first term, which is quadratic in $x$ and sums to a constant independent of the model, we arrive at the simplified expression:\n",
    "\n",
    "$$logL^{\\left(k\\right)}_{max} =\\frac{b_k^2}{4a_k}$$\n",
    "\n",
    "To summarize, comprehensive simulations were carried out to evaluate the calibration of the ncp_prior for normally distributed point measurements. These simulations encompassed varying values of $N$ (the total number of events) and the selected false positive rate denoted as $p_0$. The results obtained from these simulations were effectively fitted to the following formula:\n",
    "\n",
    "$$\\text{ncp_prior} = 1.32 + 0.577\\cdot\\text{log}\\left(N\\right)$$\n",
    "\n",
    "Importantly, this relationship remains unaffected by the signal-to-noise ratio in the simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm <a name=\"algorithm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Algorithm Description <a name=\"alg_description\"></a>\n",
    "\n",
    "We will follow a **dynamic programming** approach. Beginning with the first data cell, at each step one more cell is added using the results of the previous steps: the value of optimal fitness is stored in the array *best* and the location of the last change point in the array *last*.\n",
    "\n",
    "Let $\\mathcal{P}^{opt}(R)$ denote the optimal partition of the first $R$ cells. In the starting case $R = 1$, the only possible partition (one block consisting of the first cell by itself) is trivially optimal. Now assume we have completed step $R$: it remains to show how to obtain $\\mathcal{P}^{opt}(R+1)$.  \n",
    "For some $r$ consider the set of all partitions (of these first R+1 cells) whose last block starts with cell $r$ (and by definition ends at R + 1) and denote the fitness of this last block by $F(r)$. The only member of this set that could possibly be optimal is that consisting of $\\mathcal{P}^{opt}(r-1)$ followed by this last block. By the additivity of the Likelihood the fitness of said partition is the sum of $F(r)$ and the fitness of $\\mathcal{P}^{opt}(r-1)$ (saved from a previous step in *best*):\n",
    "\n",
    "$$ \n",
    "A(r) = F(r) + \n",
    "\\begin{cases}\n",
    "    0, & r = 1\\\\\n",
    "    best(r - 1), & r = 2, 3, . . . , R + 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The value of $r$ yielding the optimal partition $\\mathcal{P}^{opt}(R+1)$ is the value maximizing $A(r)$:\n",
    "\n",
    "$$\n",
    "r^{opt} = argmax[A(r)]\n",
    "$$\n",
    "\n",
    "\n",
    "At the end of this computation, when $R = N$, it only remains to find the locations of the change points of the optimal partition. The needed information is contained in the array *last*: we use the last value in this array to determine the last change point in $\\mathcal{P}^{opt}(N)$, peel off the end section of last corresponding to this last block, and repeat:\n",
    "\n",
    "$$\n",
    "cp1 = last(N) \\\\\n",
    "cp2 = last(cp1 − 1) \\\\\n",
    "cp3 = last(cp2 − 1) \\\\\n",
    "...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Code Implementation <a name=\"alg_implementation\"></a>\n",
    "\n",
    "Here is the R code implementing the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "bayesian_blocks <- function(data,\n",
    "                            weights = NULL,\n",
    "                            Prior = \"calibrated\",\n",
    "                            p0 = 0.05,\n",
    "                            gamma = NULL) {\n",
    "    # Bayesian Blocks Implementation.\n",
    "\n",
    "    # This is a flexible implementation of the Bayesian Blocks algorithm for 1 dimension.\n",
    "\n",
    "    # Args:\n",
    "    #   data: Input data values (one dimensional, length N). Repeat values are allowed.\n",
    "\n",
    "    #   weights: Weights for data (otherwise assume all data points have a weight of 1).\n",
    "    #            Must be same length as data. Defaults to NULL.\n",
    "\n",
    "    #   prior: Prior on the number of blocks. Options include a uniform prior, a gemetric prior that\n",
    "    #          penalizes an excessive number of blocks and a calibrated prior in order to achieve the\n",
    "    #          specified false-positive rate p0. Deafults to the calibrated one. \n",
    "\n",
    "    #   p0: False-positive rate, between 0 and 1. A lower number places a stricter penalty\n",
    "    #       against creating more bin edges, thus reducing the potential for false-positive bin edges. In general,\n",
    "    #       the larger the number of bins, the small the p0 should be to prevent the creation of spurious, jagged\n",
    "    #       bins. Defaults to 0.05.\n",
    "\n",
    "    #   gamma: If specified, can use this gamma to compute the general prior form,\n",
    "    #          p = gamma^N. Defaults to NULL.\n",
    "\n",
    "    # Returns:\n",
    "    #   Array containing the (N+1) bin edges\n",
    "\n",
    "    # Examples:\n",
    "    #   Unweighted data:\n",
    "\n",
    "    #   >>> d <- rnorm(n=100)\n",
    "    #   >>> bins <- bayesian_blocks(d, p0=0.01)\n",
    "\n",
    "    #   Unweighted data with repeats:\n",
    "\n",
    "    #   >>> d <- rnorm(n=100)\n",
    "    #   >>> d[81:100] <- d[1:20]\n",
    "    #   >>> bins <- bayesian_blocks(d, p0=0.01)\n",
    "\n",
    "    #   Weighted data:\n",
    "\n",
    "    #   >>> d <- rnorm(n=100)\n",
    "    #   >>> w <- runif(n=100, min=1, max=2)\n",
    "    #   >>> bins <- bayesian_blocks(d, w, p0=0.01)\n",
    "\n",
    "    \n",
    "    # validate input data\n",
    "    data <- as.vector(data)\n",
    "\n",
    "    # validate input weights\n",
    "    if (!is.null(weights)) {\n",
    "        weights <- as.vector(weights)\n",
    "    }\n",
    "    else {\n",
    "        # set them to 1 if not given\n",
    "        weights <- rep(1, length(data))\n",
    "    }\n",
    "\n",
    "    # Place data and weights into a DataFrame.\n",
    "    # We want to sort the data array (without losing the associated weights), and combine duplicate\n",
    "    # data points by summing their weights together.\n",
    "    df <- data.frame(data = data, weights = weights) |>\n",
    "          group_by(data) |>\n",
    "          summarise(sum_w = sum(weights))\n",
    "    data <- pull(df, data)\n",
    "    weights <- pull(df, sum_w)\n",
    "\n",
    "    N = length(data)\n",
    "\n",
    "\n",
    "    # create length-(N + 1) array of cell edges\n",
    "    edges <- c(data[1], 0.5 * (data[1:(N-1)] + data[2:N]), data[N])\n",
    "    block_length = data[N] - edges\n",
    "\n",
    "\n",
    "    # arrays to store the best configuration\n",
    "    best <- rep(0, N)\n",
    "    last <- rep(0, N)\n",
    "\n",
    "\n",
    "    # compute prior\n",
    "    prior <- switch(Prior, \n",
    "                    \"uniform\" = 0,\n",
    "                    \"gamma\" = -log(gamma),\n",
    "                    \"calibrated\" = 4 - log(73.53 * p0 * N**(-0.478)),\n",
    "                    0 # default case\n",
    "             )\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Start with first data cell; add one cell at each iteration\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    for (R in 1:N) {\n",
    "        # Compute fit_vec : fitness of putative last block (ends at data cell R/edge R+1)\n",
    "\n",
    "        # T_k: width/duration of each block\n",
    "        T_k <- block_length[1:R] - block_length[R+1]\n",
    "\n",
    "        # N_k: number of elements in each block\n",
    "        N_k <- rev(cumsum(rev(weights[1:R])))\n",
    "\n",
    "        # evaluate fitness function using Cash statistics\n",
    "        fit_vec <- N_k * log(N_k / T_k)\n",
    "\n",
    "        # penalize function with prior\n",
    "        A_R <- fit_vec - prior\n",
    "        if(R > 1) {\n",
    "            A_R[2:R] <- A_R[2:R] + best[1:(R-1)]\n",
    "        }\n",
    "        i_max = which.max(A_R)\n",
    "        last[R] = i_max\n",
    "        best[R] = A_R[i_max]\n",
    "    }\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Now find changepoints by iteratively peeling off the last block\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    change_points = rep(0, N)\n",
    "    i_cp = N + 1\n",
    "    ind = N + 1\n",
    "    repeat {\n",
    "        i_cp <- i_cp - 1\n",
    "        change_points[i_cp] <- ind\n",
    "        if(ind == 1) {\n",
    "            break\n",
    "        }\n",
    "        ind <- last[ind-1]\n",
    "    }\n",
    "    change_points = change_points[i_cp:N]\n",
    "\n",
    "    return(edges[change_points])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Execution Time <a name=\"alg_time\"></a>\n",
    "\n",
    "The total number of possible partitions (i.e., the number of ways N cells can be arranged in blocks) is $2^N$, rendering an explicit exhaustive search of partition space impossible in the general case. This algorithm, instead, is able to find the optimal partition in time of order $O(N^2)$, and is practical even for $N ∼ 1,000,000$, for which approximately $10^{300,000}$ partitions are possible.  \n",
    "\n",
    "We will verify this scaling law by computing the execution time of the algorithm over datasets of increasing size over different order of magnitude. For simplicity, the data is generated form a Normal distribution \n",
    "${\\mathcal {N}}(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Test execution time\n",
    "compute_time <- Vectorize(function(N, func, time_unit = \"sec\", seed = 48) {\n",
    "    # create dummy dataset\n",
    "    set.seed(seed)\n",
    "    data <- rnorm(N, mean = 0, sd = 1)\n",
    "\n",
    "    # start clock\n",
    "    start <- Sys.time()\n",
    "\n",
    "    func(data)\n",
    "\n",
    "    # end clock\n",
    "    end <- Sys.time()\n",
    "    exec_time <- as.numeric(difftime(end, start, units = time_unit))\n",
    "    return(exec_time)\n",
    "}, vectorize.args = \"N\") \n",
    "\n",
    "\n",
    "# Compute execution time over different ranges\n",
    "N1 <- c(1e2, 1e3, 2e3, 3e3, 4e3, 5e3, 6e3, 7e3, 8e3, 9e3, 1e4)\n",
    "N2 <- c(1e3, round(sqrt(1e7)), 1e4, round(sqrt(1e9)), 1e5)\n",
    "\n",
    "exec_time1 <- compute_time(N1, bayesian_blocks)\n",
    "exec_time2 <- compute_time(N2, bayesian_blocks)\n",
    "\n",
    "\n",
    "# Quadratic model fit\n",
    "fit_data1 <- data.frame(x = N1, x2 = N1**2, y = exec_time1)\n",
    "fit_data2 <- data.frame(x = N2, x2 = N2**2, y = exec_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "fitted_func <- function(fit, x) {\n",
    "    res <- as.numeric(fit$coefficients[\"(Intercept)\"]) +\n",
    "           as.numeric(fit$coefficients[\"x\"]) * x +\n",
    "           as.numeric(fit$coefficients[\"x2\"]) * x**2\n",
    "    return(res)\n",
    "}\n",
    "\n",
    "par(mfrow = c(1, 2))\n",
    "options(repr.plot.width=16, repr.plot.height=8)\n",
    "\n",
    "plot(N1, exec_time1, xlab = \"N\", ylab = \"Execution Time [s]\", col = \"blue\", pch = 10, lwd = 3)\n",
    "grid()\n",
    "curve(fitted_func(fit1, x), N1[1], N1[length(N1)],\n",
    "      col = \"red\", lw = 2, add = T)\n",
    "legend(\"topleft\", inset=+0.1, legend = c(\"Points\", \"Quadratic Fit\"),\n",
    "      col = c(\"blue\",\"red\"), lty = c(NA, 1), pch = c(10, NA), lwd = c(3, 2),\n",
    "      cex = 0.9, box.lty = 0, border = F)\n",
    "\n",
    "plot(N2, exec_time2, xlab = \"N\", ylab = \"Execution Time [s]\", col = \"blue\", pch = 10, lwd = 3, log = \"xy\")\n",
    "grid()\n",
    "curve(fitted_func(fit2, x), N2[1], N2[length(N2)],\n",
    "      col = \"red\", lw = 2, add = T)\n",
    "legend(\"topleft\", inset=+0.1, legend = c(\"Points\", \"Quadratic Fit\"),\n",
    "      col = c(\"blue\",\"red\"), lty = c(NA, 1), pch = c(10, NA), lwd = c(3, 2),\n",
    "      cex = 0.9, box.lty = 0, border = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the data points follow a quadratic distribution quite nicely, with some slight deviations at lower $N$ for linear and constant effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison with Other Binning Methods <a name=\"comparison\"></a>\n",
    "\n",
    "Other objective methods have been proposed to determine binning according to some optimization procedure.\n",
    "We will now compare the Bayesian Block algorithm using different priors with the following:\n",
    "\n",
    "- Square root: $K = \\lceil \\sqrt{N} \\rceil$\n",
    "- Sturges: $K = \\lceil ln(N) \\rceil + 1$  [[3]](#Sturges)\n",
    "- Freedman-Diaconis: $h = 2 \\frac{IQRi}{N^{1/3}}$, where $IQR$ is the interquartile range of the data  [[4]](#FreedmanDiaconis)\n",
    "\n",
    "where $K$ is the number of bins and $h$ is their width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Metrics <a name=\"comp_metrics\"></a>\n",
    "\n",
    "In order to quantitavely compare different methods we will use two metrics, introduced by Pollack. [[2]](#Pollack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Wiggles metric\n",
    "\n",
    "This metric is designed to capture the visual appeal of the histogram by minimizing the number of bin-to-bin height fluctuations, hereafter referred to as \"wiggles\", that denote unwanted statistical fluctuations. The number of wiggles in a histogram is defined as:\n",
    "\n",
    "$$ W_n = \\sum \\left[ sgn \\left( f'\\left(B_i \\right) \\right) \\cdot sgn \\left( f'\\left(B_{i+1} \\right) \\right) = -1 \\right]$$\n",
    "\n",
    "where $f'\\left(B_i \\right)$ is the finite first derivative of the function describing the height of block (or bin) $i$.\n",
    "\n",
    "This metric simply counts the number of adjacent opposite-sign first derivatives and increases whene there are many fluctuations in height form one to another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Wiggle <- function(data, breaks, numBlock) {\n",
    "    histogram <- hist(data, breaks = breaks)\n",
    "    trash <- dev.off()\n",
    "\n",
    "    bin_counts <- histogram$counts\n",
    "    post_Wiggling <- bin_counts[numBlock+1] - bin_counts[numBlock]\n",
    "    return(post_Wiggling)\n",
    "}\n",
    "\n",
    "\n",
    "Wiggle_metric <- function(data, breaks) {\n",
    "  \n",
    "    # Wiggle metric Implementation.\n",
    "    #\n",
    "    # Args:\n",
    "    #   data: Input data values (one dimensional, length N). Repeat values are allowed.\n",
    "    #\n",
    "    #   breaks: Sequence of change points or bin edges fro the input data\n",
    "    #\n",
    "    # Returns:\n",
    "    #   W_n, the metric computation\n",
    "    #\n",
    "  \n",
    "    numBlocks <- length(breaks) - 1\n",
    "  \n",
    "    W_n <- 0\n",
    "    for (i in 1:(numBlocks-2)) {\n",
    "        f.prime_B1 <- Wiggle(data, breaks, i)\n",
    "        f.prime_B2 <- Wiggle(data, breaks, i+1)\n",
    "    \n",
    "        prod <- sign(f.prime_B1)*sign(f.prime_B2)\n",
    "        if (prod < 0) {\n",
    "            W_n <- W_n + 1\n",
    "        }\n",
    "      }\n",
    "  return(W_n)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Average error metric\n",
    "\n",
    "This metric measures the accuracy of a given histogram in reconstructing the underlying pdf, while minimizing the impact of statistical fluctuations due to the initial data used to generate the histogram.\n",
    "\n",
    "Consider a dataset that consists of N independent events such that $D = \\{ d_i\\}_{i=1,...,N}$. From it construct a histogram which allows to generate a new dataset $\\hat{D}$, where each point is generated by a linear interpolation of each bin. This can be done by considering each bin $i$ and extracting for each of them $n_i$ equally spaced points, where $n_i$ is equal to the height of the bin. Thus $\\hat{D}$ is equal in size to the original data, but evenly distributed within each respective bin.\n",
    "\n",
    "Then one can compare the interpolated dataset with M different independent datasets, all of which are derived from the original distribution and have size N. We can use these datasets to construct the average metric error, defined as:\n",
    "\n",
    "$$ \\hat{E} = \\frac{1}{M}\\sum^M_{m=1} \\left(\\sum^N_{n=1} |d_{nm}-\\hat{d}_n|\\right) $$\n",
    "\n",
    "where $d_{nm}$ is the n-th data point from the m-th data set.\n",
    "\n",
    "This metric typically decreases as the size of the bins decreases, but in general does not approach 0 as the bins become infinitesimally narrow. The metric penalizes a histogram for modeling the statistical fluctuations of a given distribution by comparing the interpolated data with statistically independent datasets and not the dataset used to generate the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Average_metric<-function(breaks, data, distribution, M=20, ...) {\n",
    "\n",
    "    # Average_metric Implementation.\n",
    "    #\n",
    "    # Args:\n",
    "    #   data: Input data values (one dimensional, length N). Repeat values are allowed.\n",
    "    #\n",
    "    #   breaks: Sequence of change points or bin edges fro the input data\n",
    "    #\n",
    "    #   distribution: underlying distributionused to generate toy dataset\n",
    "    #\n",
    "    #   M: number of independent toy dataset generated\n",
    "    #\n",
    "    #   ...: arguments to generate underlying distribution\n",
    "    #\n",
    "    # Returns:\n",
    "    #   E_hat\n",
    "    #\n",
    "  \n",
    "    histogram <- hist(data, breaks = breaks)\n",
    "    trash <- dev.off()\n",
    "  \n",
    "    numBlocks <- length(breaks)-1\n",
    "    bin_counts <- histogram$counts\n",
    "    \n",
    "    hat_data <- c()\n",
    "    for (i in 1:numBlocks) {\n",
    "        points <- seq(breaks[i], breaks[i+1], length.out=bin_counts[i])\n",
    "        hat_data <- c(hat_data, points)\n",
    "    }\n",
    "  \n",
    "    N <- length(data)\n",
    "    Mdata <- matrix(nrow = N, ncol = M)\n",
    "  \n",
    "    for(i in 1:M) {\n",
    "        Mdata[,i] <- distribution(N, ...)\n",
    "    }\n",
    "  \n",
    "    E_hat <- 0\n",
    "    for (m in 1:M) {\n",
    "        for(n in 1:N) {\n",
    "            E_hat <- E_hat + abs(Mdata[n,m] - hat_data[n])\n",
    "        }\n",
    "    }\n",
    "  \n",
    "    E_hat <- E_hat/M\n",
    "  \n",
    "    return(E_hat)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Results <a name=\"comp_results\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applications to Nuclear and Subnuclear Physics  <a name=\"applications\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Radioactive Source  Spectrum <a name=\"app_radio\"></a>\n",
    "\n",
    "We will apply the algorithm to energy spectra measured by a $LaBr_3$ scintillator detector at INFN Legnaro laboratories. The following raioactive sources will be studied:\n",
    "\n",
    "- Cobalt-60\n",
    "- Cesium-137\n",
    "- Sodium-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m700325\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m2\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \"\\t\"\n",
      "\u001b[32mdbl\u001b[39m (2): ADC_channel, Energy\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "\u001b[1mRows: \u001b[22m\u001b[34m542364\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m2\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \"\\t\"\n",
      "\u001b[32mdbl\u001b[39m (2): ADC_channel, Energy\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "\u001b[1mRows: \u001b[22m\u001b[34m572864\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m2\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \"\\t\"\n",
      "\u001b[32mdbl\u001b[39m (2): ADC_channel, Energy\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "Co <- read_tsv(\"Data/Co10cm.txt\") |> pull(\"Energy\")\n",
    "Cs <- read_tsv(\"Data/Cs10cm.txt\") |> pull(\"Energy\")\n",
    "Na <- read_tsv(\"Data/Na10cm.txt\") |> pull(\"Energy\")\n",
    "\n",
    "breaks_Co <- bayesian_blocks(Co)\n",
    "breaks_Cs <- bayesian_blocks(Cs)\n",
    "breaks_Na <- bayesian_blocks(Na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "par(mfrow = c(3, 1))\n",
    "options(repr.plot.width = 8, repr.plot.height=24)\n",
    "\n",
    "hist(Co, breaks = breaks_Co, col = \"red\",  border = FALSE, density = 100,\n",
    "     xlab = \"Energy [KeV]\", ylab = \"Counts\", main = sprintf(\"Co-60\"))\n",
    "grid()\n",
    "hist(Cs, breaks = breaks_Cs, col = \"blue\", border = FALSE, density = 100,\n",
    "     xlab = \"Energy [KeV]\", ylab = \"Counts\", main = sprintf(\"Cs-137\")) \n",
    "grid()\n",
    "hist(Na, breaks = breaks_na, col = \"blue\", border = FALSE, density = 100,\n",
    "     xlab = \"Energy [KeV]\", ylab = \"Counts\", main = sprintf(\"Na-22\")) \n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Higgs Physics <a name=\"app_higgs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bibliography <a name=\"bibliography\"></a>\n",
    "\n",
    "[1] J. D. Scargle et al.,\n",
    "    Astrophys. J. **764** (2013) 167\n",
    "    <a name=\"Scargle\"></a>\n",
    "\n",
    "[2] B. Pollack *et al.*,\n",
    "    [arXiv:1708.008 10](https://arxiv.org/abs/1708.00810)\n",
    "    <a name=\"Pollack\"></a>\n",
    "\n",
    "[3] H. A. Sturges,\n",
    "    Journal of the American Statistical Association  **21**, 65 (1926),\n",
    "    https://doi.org/10.1080/01621459.1926.10502161.\n",
    "    <a name=\"Sturges\"></a>\n",
    "                                             \n",
    "[4] D. Freedman and P. Daiconis,\n",
    "    Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete **57**, 453 (1981).\n",
    "    <a name=\"FreedmanDiaconis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(48)\n",
    "norm_data <- c(rnorm(2000,0,0.8),rnorm(2000,5,0.7),rnorm(2000,7,0.5))\n",
    "N <- length(norm_data)\n",
    "breaks = bayesian_blocks(norm_data, Prior=\"calibrated\", p0 = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "hist(norm_data,breaks=\"freedman-diaconis\",freq=FALSE,col=scales::alpha('darkblue',.55),border=F, panel.first=grid(),density=200,\n",
    "     xlab='x',ylab='Density',main=sprintf('Histogram of Normal distributed variables'))\n",
    "g <- hist(norm_data,breaks=breaks,density=200,\n",
    "     freq=FALSE,col=scales::alpha('firebrick3',.5),border=F,add=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#exec_time1 <- c(0.0172479152679443, 0.133802890777588, 0.338721036911011, 0.768672943115234, \n",
    "#                1.12476801872253, 1.64819598197937, 2.45492100715637, 2.97475099563599, 3.66642284393311,\n",
    "#                4.35661911964417, 5.0487699508667)\n",
    "#exec_time2 <- c(0.0695300102233887, 0.577858924865723, 4.86060404777527, 47.8130168914795, 470.706510066986)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
